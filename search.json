[
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5"
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5"
  },
  {
    "objectID": "Projects/W07 - U3 - Week B - Class Code Walkthrough 1.html",
    "href": "Projects/W07 - U3 - Week B - Class Code Walkthrough 1.html",
    "title": "W07.1 - U3 (Week B) - Class Code",
    "section": "",
    "text": "Introduction to the functions you will be learning in this unit. Below is the code used in the video so you can follow along.\n\n# %%\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n# %%\n# Upgrade pip if needed\nimport sys\n!{sys.executable} -m pip install --upgrade pip\n\nRequirement already satisfied: pip in c:\\users\\14064\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (24.0)\n\n\n\n# %%\n# careful to list your path to the file. Or save this code in the same folder as the sqlite file.\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\n# %%\nq = 'SELECT * FROM batting LIMIT 100'\nbatting5 = pd.read_sql_query(q,con)\n\n\n# %%\nbatting5\n\n\n\n\n\n\n\n\nID\nplayerID\nyearID\nstint\nteamID\nteam_ID\nlgID\nG\nG_batting\nAB\n...\nRBI\nSB\nCS\nBB\nSO\nIBB\nHBP\nSH\nSF\nGIDP\n\n\n\n\n0\n1\nabercda01\n1871\n1\nTRO\n8\nNA\n1\nNone\n4\n...\n0\n0\n0\n0\n0\nNone\nNone\nNone\nNone\n0\n\n\n1\n2\naddybo01\n1871\n1\nRC1\n7\nNA\n25\nNone\n118\n...\n13\n8\n1\n4\n0\nNone\nNone\nNone\nNone\n0\n\n\n2\n3\nallisar01\n1871\n1\nCL1\n3\nNA\n29\nNone\n137\n...\n19\n3\n1\n2\n5\nNone\nNone\nNone\nNone\n1\n\n\n3\n4\nallisdo01\n1871\n1\nWS3\n9\nNA\n27\nNone\n133\n...\n27\n1\n1\n0\n2\nNone\nNone\nNone\nNone\n0\n\n\n4\n5\nansonca01\n1871\n1\nRC1\n7\nNA\n25\nNone\n120\n...\n16\n6\n2\n2\n1\nNone\nNone\nNone\nNone\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n96\nsenseco01\n1871\n1\nPH1\n6\nNA\n25\nNone\n127\n...\n23\n5\n3\n0\n1\nNone\nNone\nNone\nNone\n0\n\n\n96\n97\nsimmojo01\n1871\n1\nCH1\n2\nNA\n27\nNone\n129\n...\n17\n4\n1\n1\n0\nNone\nNone\nNone\nNone\n2\n\n\n97\n98\nsmithch01\n1871\n1\nNY2\n5\nNA\n14\nNone\n72\n...\n5\n6\n0\n1\n1\nNone\nNone\nNone\nNone\n0\n\n\n98\n99\nspaldal01\n1871\n1\nBS1\n1\nNA\n31\nNone\n144\n...\n31\n2\n0\n8\n1\nNone\nNone\nNone\nNone\n0\n\n\n99\n100\nstartjo01\n1871\n1\nNY2\n5\nNA\n33\nNone\n161\n...\n34\n4\n2\n3\n0\nNone\nNone\nNone\nNone\n0\n\n\n\n\n100 rows × 25 columns\n\n\n\n\n# %%\n# What columns do we want to select?\nq = '''\nSELECT  playerid, \n        teamid, \n        ab, \n        r\nFROM batting\nLIMIT 5\n'''\n\npd.read_sql_query(q,con)\n\n\n\n\n\n\n\n\nplayerID\nteamID\nAB\nR\n\n\n\n\n0\nabercda01\nTRO\n4\n0\n\n\n1\naddybo01\nRC1\n118\n30\n\n\n2\nallisar01\nCL1\n137\n28\n\n\n3\nallisdo01\nWS3\n133\n28\n\n\n4\nansonca01\nRC1\n120\n29\n\n\n\n\n\n\n\n\n# %%\n# What calculation do we want to perform?\n\nq = '''\nSELECT playerid, \n        teamid, \n        ab, \n        r, \n        r*(1.0)/ab as 'ba'\nFROM batting\nLIMIT 100\n'''\n\nbatting_calc = pd.read_sql_query(q,con)\nbatting_calc\n\n\n\n\n\n\n\n\nplayerID\nteamID\nAB\nR\nba\n\n\n\n\n0\nabercda01\nTRO\n4\n0\n0.000000\n\n\n1\naddybo01\nRC1\n118\n30\n0.254237\n\n\n2\nallisar01\nCL1\n137\n28\n0.204380\n\n\n3\nallisdo01\nWS3\n133\n28\n0.210526\n\n\n4\nansonca01\nRC1\n120\n29\n0.241667\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\nsenseco01\nPH1\n127\n38\n0.299213\n\n\n96\nsimmojo01\nCH1\n129\n29\n0.224806\n\n\n97\nsmithch01\nNY2\n72\n15\n0.208333\n\n\n98\nspaldal01\nBS1\n144\n43\n0.298611\n\n\n99\nstartjo01\nNY2\n161\n35\n0.217391\n\n\n\n\n100 rows × 5 columns\n\n\n\n\n# %%\n# What name do we give our calculated column?\n\nq = '''\nSELECT teamid, \n        Sum(r) as Total_Runs\nFROM batting\nGROUP BY teamid\nLIMIT 10000\n'''\n\nbatting_calc = pd.read_sql_query(q,con)\nbatting_calc\n\n\n\n\n\n\n\n\nteamID\nTotal_Runs\n\n\n\n\n0\nALT\n90\n\n\n1\nANA\n6305\n\n\n2\nARI\n16223\n\n\n3\nATL\n37588\n\n\n4\nBAL\n46033\n\n\n...\n...\n...\n\n\n144\nWS6\n107\n\n\n145\nWS7\n248\n\n\n146\nWS8\n2160\n\n\n147\nWS9\n691\n\n\n148\nWSU\n572\n\n\n\n\n149 rows × 2 columns\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Client Report - Can you predict that?",
    "section": "",
    "text": "Code\n# %% \n# install packages\nimport sys\n!{sys.executable} -m pip install scikit-learn\n\n\nRequirement already satisfied: scikit-learn in c:\\users\\14064\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.2)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.17.3 in c:\\users\\14064\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.26.3)\nRequirement already satisfied: scipy&gt;=1.5.0 in c:\\users\\14064\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib&gt;=1.1.1 in c:\\users\\14064\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in c:\\users\\14064\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.2.0)\nCode\n# %% \n# scikit learn froms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nCode\n# %%\n# load data\ndwellings_ml = pd.read_csv(\"https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv\")"
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Client Report - Can you predict that?",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nI’ve been exploring our dataset on housing variables and their construction years, and I’ve identified some intriguing patterns. By visualizing the correlation matrix and scatter matrix of relevant home features, I’ve gained insights into the relationships between these variables and the construction year. Additionally, I’ve built a classification model to predict whether a house was built before or after 1980 with over 90% accuracy, leveraging important features identified by the model."
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\nThese charts are very good at showing the differneces in the years of houses. By coloring the points based on the ‘before1980’ variable, it helps identify any patterns or differences in the relationships between the variables for homes built before 1980 and those built after. You can see the patterns that houses before 1980 had from the houses built after. The second chart, the correlation heatmap, is helpful for understanding the linear relationship between pairs of variables.\n\n\nPrepare and visualize a scatter matrix of selected home variables\nh_subset = dwellings_ml.filter(\n    ['livearea', 'finbsmnt', 'basement', \n    'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 'before1980',\n    'stories', 'yrbuilt']).sample(500)\nchart = px.scatter_matrix(h_subset,\n    dimensions=['livearea', 'finbsmnt', 'basement'],\n    color='before1980'\n)\nchart.update_traces(diagonal_visible=False)\nchart.show()\n\n\n\n                                                \n\n\n\n\nVisualize the correlation matrix of home variables\ncorr = h_subset.drop(columns = 'before1980').corr()\n\npx.imshow(corr,text_auto=True)\n\n\n\n                                                \nCorrelation Matrix of Home Variables"
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI chose a decision tree classifier as my model. After training the decision tree classifier on the training data, I evaluated its performance on the test data using accuracy as the metric. The accuracy got up to 0.9005262482351432. The way that I got it there was by filtering out the rows that were not helping much and keeping the top 6 rows because that hepled train it the best.\n\n\nPrepare data for classification by dropping irrelevant columns, selecting features, and splitting into train and test sets\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 76)  \n\n\n\n\nPrepare data for classification by selecting relevant features and splitting into train and test sets\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\ndf_features = pd.DataFrame(\n    {'f_names': X_train.columns} \n)\n\nX_pred = dwellings_ml.filter(dwellings_ml.filter(list(df_features.f_names.head(6))).columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 76) \n\n\n\n\nEvaluate the accuracy of the decision tree classifier\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(metrics.accuracy_score(y_pred, y_test))\n\n\n0.8647156975997946"
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nI examined the importance of each feature in predicting whether a house was built before 1980 or during/after 1980. Using the feature_importances_ attribute of the trained decision tree classifier, I ranked the features based on their importance. You can see that livearea and arcstyle_ONE-STORY are both combining for over 55% of the training alone. The other 4 also provide the rest of what is important.\n\n\nExtract and visualize feature importances from the trained decision tree classifier\ndf_features = pd.DataFrame(\n    {'f_names': X_train.columns, \n    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)\n\nchart = px.bar(df_features.head(10),\n    x='f_values', \n    y='f_names'\n)\n\nchart.update_layout(yaxis={'categoryorder':'total ascending'})"
  },
  {
    "objectID": "Projects/project4.html#questiontask-4",
    "href": "Projects/project4.html#questiontask-4",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nTo assess the quality of my classification model I used several evaluation metrics, including accuracy, precision, recall, and F1-score. The most useful of these would be the accuracy. It came out to be .90 accurate showing that 90% of the predictions were correct. Precision measures the proportion of true positive predictions out of all positive predictions made by the model. Recall measures the proportion of true positive predictions out of all actual positive instances. F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n\n\nEvaluate classification model and generate a classification report\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(metrics.classification_report(y_pred, y_test))\n\n\n              precision    recall  f1-score   support\n\n           0       0.85      0.80      0.82      3071\n           1       0.87      0.91      0.89      4720\n\n    accuracy                           0.86      7791\n   macro avg       0.86      0.85      0.86      7791\nweighted avg       0.86      0.86      0.86      7791"
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Client Report - Late flights and missing data",
    "section": "",
    "text": "Hey, did you know that digging into flight data can unveil some fascinating insights? I recently came across a project where they analyzed delay patterns across airports, and it’s pretty eye opening. Turns out, some airports are way more prone to delays than others. But here’s the kicker, they also pinpointed the best month for flying to minimize disruptions. They even looked into weather related delays and how they vary across different airports.\n\n\nRead and format project data\n# Include and execute your code here\nurl_flights = 'https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json'\nflights = pd.read_json(url_flights)"
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Client Report - Late flights and missing data",
    "section": "",
    "text": "Hey, did you know that digging into flight data can unveil some fascinating insights? I recently came across a project where they analyzed delay patterns across airports, and it’s pretty eye opening. Turns out, some airports are way more prone to delays than others. But here’s the kicker, they also pinpointed the best month for flying to minimize disruptions. They even looked into weather related delays and how they vary across different airports.\n\n\nRead and format project data\n# Include and execute your code here\nurl_flights = 'https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json'\nflights = pd.read_json(url_flights)"
  },
  {
    "objectID": "Projects/project2.html#questiontask-1",
    "href": "Projects/project2.html#questiontask-1",
    "title": "Client Report - Late flights and missing data",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.__\n\n\nRead and format data\n# Include and execute your code here\n\nflights.replace(-999, pd.NA, inplace=True)\n\n\n\n\nCode\nvalue = flights.iloc[0]\nvalue\n\n\nairport_code                                                                   ATL\nairport_name                     Atlanta, GA: Hartsfield-Jackson Atlanta Intern...\nmonth                                                                      January\nyear                                                                        2005.0\nnum_of_flights_total                                                         35048\nnum_of_delays_carrier                                                        1500+\nnum_of_delays_late_aircraft                                                   &lt;NA&gt;\nnum_of_delays_nas                                                             4598\nnum_of_delays_security                                                          10\nnum_of_delays_weather                                                          448\nnum_of_delays_total                                                           8355\nminutes_delayed_carrier                                                   116423.0\nminutes_delayed_late_aircraft                                               104415\nminutes_delayed_nas                                                       207467.0\nminutes_delayed_security                                                       297\nminutes_delayed_weather                                                      36931\nminutes_delayed_total                                                       465533\nName: 0, dtype: object"
  },
  {
    "objectID": "Projects/project2.html#questiontask-2",
    "href": "Projects/project2.html#questiontask-2",
    "title": "Client Report - Late flights and missing data",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nThe airport with the worst delays is The Chicago O’Hare if you account for the total delayed time. If you want the total delays then Atlanata would be the worst.\n\n\nRead and format data\n# Include and execute your code here\n\n\ngrouped = flights.groupby('airport_code')\n\n# Calculate total number of flights for each airport\ntotal_flights = grouped['num_of_flights_total'].sum()\n\n# Calculate total number of delayed flights for each airport\ntotal_delayed_flights = grouped['num_of_delays_total'].sum()\n\n# Calculate proportion of delayed flights for each airport\nproportion_delayed = total_delayed_flights / total_flights\n\n# Calculate average delay time for each airport\naverage_delay_time = grouped['minutes_delayed_total'].sum() / total_delayed_flights / 60  # Convert minutes to hours\n\n# Create summary table\nsummary_table = pd.DataFrame({\n    'Total Flights': total_flights,\n    'Delayed Flights': total_delayed_flights,\n    'Proportion Delayed': proportion_delayed,\n    'Average Delay Time (hours)': average_delay_time\n})\n\n# Sort summary table by proportion of delayed flights\nsummary_table = summary_table.sort_values(by='Average Delay Time (hours)', ascending=False)\n\n# Display summary table\nsummary_table\n\n\n\n\n\n\n\n\n\nTotal Flights\nDelayed Flights\nProportion Delayed\nAverage Delay Time (hours)\n\n\nairport_code\n\n\n\n\n\n\n\n\nORD\n3597588\n830825\n0.230939\n1.130525\n\n\nSFO\n1630945\n425604\n0.260955\n1.039718\n\n\nIAD\n851571\n168467\n0.197831\n1.017358\n\n\nATL\n4430047\n902443\n0.203710\n0.996996\n\n\nDEN\n2513974\n468519\n0.186366\n0.895495\n\n\nSLC\n1403384\n205160\n0.146189\n0.822396\n\n\nSAN\n917862\n175132\n0.190804\n0.787620"
  },
  {
    "objectID": "Projects/project2.html#questiontask-3",
    "href": "Projects/project2.html#questiontask-3",
    "title": "Client Report - Late flights and missing data",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length? Discuss the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\nIf you wanted to avoid delays of any length you should try to fly in November. The metric that I chose was the total delays that an airport has over the month. I chose that metric because it seems to be one of the most applicable metrics to use for this question.\n\n\nCalculate total delays for each month\n# Include and execute your code here\n\n# Remove rows with missing Month variable\nflights = flights.dropna(subset=['month'])\n\n# Group by month and calculate total delays\ntotal_delays_per_month = flights.groupby('month')['num_of_delays_total'].sum().reset_index()\n\n# Sort months in chronological order\nmonths_order = [\"January\", \"Febuary\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\ntotal_delays_per_month['month'] = pd.Categorical(total_delays_per_month['month'], categories=months_order, ordered=True)\ntotal_delays_per_month = total_delays_per_month.sort_values('month')\n\n\n\n\nPlot total delays per month\n# Include and execute your code here\n\nfig = px.bar(total_delays_per_month, x='month', y='num_of_delays_total', \n             title='Total Delays per Month',\n             labels={'num_of_delays_total': 'Total Delays', 'month': 'Month'})\nfig.show()\n\n\n\n                                                \nTotal Delays per Month"
  },
  {
    "objectID": "Projects/project2.html#questiontask-4",
    "href": "Projects/project2.html#questiontask-4",
    "title": "Client Report - Late flights and missing data",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:__\n100% of delayed flights in the Weather category are due to weather\n30% of all delayed flights in the Late-Arriving category are due to weather.\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\n\n\nCode\n# Calculate the total number of flights delayed by weather based on the given rules\ndef calculate_weather_delay(row):\n    if row['num_of_delays_weather'] &gt; 0:\n        return row['num_of_delays_weather']\n    else:\n        late_aircraft_delay = row['num_of_delays_late_aircraft'] * 0.30\n        nas_delay = row['num_of_delays_nas'] * 0.40 if row['month'] in ['April', 'May', 'June', 'July', 'August'] else row['num_of_delays_nas'] * 0.65\n        return late_aircraft_delay + nas_delay\n\n# Apply the function to create a new column for weather delays\nflights['num_of_delays_weather_combined'] = flights.apply(calculate_weather_delay, axis=1)\n\n# Print the first 5 rows of data to show the new column\nflights.head()\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\nnum_of_delays_weather_combined\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\n1500+\n&lt;NA&gt;\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n448\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n233\n\n\n2\nIAD\n\nJanuary\n2005.0\n12381\n414\n1058\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n61\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\nJanuary\n2005.0\n28194\n1197\n2255\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n306\n\n\n4\nSAN\nSan Diego, CA: San Diego International\nJanuary\n2005.0\n7283\n572\n680\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n56"
  },
  {
    "objectID": "Projects/project2.html#questiontask-5",
    "href": "Projects/project2.html#questiontask-5",
    "title": "Client Report - Late flights and missing data",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nThe graph shows the Atlanta airport is not a good airport to go to because it has mnay delays. On the other hand, the San Diego airport is a good one because it has limited delays.\n\n\nCode\n# Calculate the total number of flights delayed by weather for each airport\nweather_delay_by_airport = flights.groupby('airport_code')['num_of_delays_weather_combined'].sum()\n\n# Calculate the total number of flights for each airport\ntotal_flights_by_airport = flights.groupby('airport_code')['num_of_flights_total'].sum()\n\n# Calculate the proportion of flights delayed by weather for each airport\nproportion_weather_delay_by_airport = weather_delay_by_airport / total_flights_by_airport\n\n# Create a barplot\nfig = px.bar(proportion_weather_delay_by_airport, x=proportion_weather_delay_by_airport.index, y=proportion_weather_delay_by_airport.values,\n             title='Proportion of Flights Delayed by Weather at Each Airport')\nfig.show()"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Client Report - What’s in a name?",
    "section": "",
    "text": "Data has a wide range of different things that it can do in our everyday lives. Naming your kids has more data and patterns than you would have ever thought about beofore. These observable patterns are important to find and understand so we can make then most of them. From kids being named things due to certain movies or even from the bible, data has a story to tell.\n\n\ndefining df\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Client Report - What’s in a name?",
    "section": "",
    "text": "Data has a wide range of different things that it can do in our everyday lives. Naming your kids has more data and patterns than you would have ever thought about beofore. These observable patterns are important to find and understand so we can make then most of them. From kids being named things due to certain movies or even from the bible, data has a story to tell.\n\n\ndefining df\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project1.html#questiontask-1",
    "href": "Projects/project1.html#questiontask-1",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nMy name, Caleb, holds an intriguing history in terms of its popularity over time. In the year of my birth, 2002, it reached its peak with 11,655 thousand kids being named Caleb. Before that period, the name wasn’t as widely utilized. However, in the years following my birth, its popularity has been on a gradual decline.\n\n\nSets birth year and all tie births to variables\n# Include and execute your code here\n\nbirth = df.query('name == \"Caleb\" and year == 2002')\nalltime = df.query('name == \"Caleb\"')\n\n\n\n\ndisplaying totals of kids named caleb in 2002\n# Include and execute your code here\ndisplay(birth.filter([\"year\",\"name\",\"Total\"]))\n\n\n\n\n\n\n\n\n\nyear\nname\nTotal\n\n\n\n\n57024\n2002\nCaleb\n11655.0\n\n\n\n\n\n2002 Total kids named Caleb\n\n\n\n\ntable of the name Caleb and its history\nfig = px.bar(alltime, x='year', y='Total')\nfig.add_vline(x=2002)"
  },
  {
    "objectID": "Projects/project1.html#questiontask-2",
    "href": "Projects/project1.html#questiontask-2",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIf I talked to someone on the phone named Brittany there would be many assumptions that I make about her age. First thing I would guess is that she or he is about 33-34 based on the fact that the name Brittany peaked in 1990 so depending on the month they would fall into that category. I would be surprised if their age was anywhere older than 40 or younger than 24.\n\n\ndefining the query of people named Brittany and getting the year and total correct\nbrit = df.query('name == \"Brittany\"').filter(['name','year','Total'])\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot of when brittany was a popular name\nfig = px.bar(brit, x='year', y='Total')\nfig.add_vline(x=1990)\n\n\n\n                                                \nUse of the Name Brittany"
  },
  {
    "objectID": "Projects/project1.html#questiontask-3",
    "href": "Projects/project1.html#questiontask-3",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nThe notable trends through the data is that the name Mary has always been the most popular name of them. The names have gone up and down over time but in the past 45 years the names have started to become less and less common. 1946-1957 was by far the most popular time for the names to be given.\n\n\nputting names into a list\nnames = ['Mary', 'Martha', 'Peter', 'Paul']\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nFiltering the correct data for the names\nfiltered_data = df[df['name'].isin(names) & (df['year'] &gt;= 1920) & (df['year'] &lt;= 2000)]\n\n\n\n\nPlotting the names and showing the differences that they have over time\nfig = px.bar(filtered_data, x='year', y='Total', color='name',\n             labels={'year': 'Year', 'Total': 'Total Occurrences'},\n             color_discrete_map={'Mary': 'blue', 'Martha': 'red', 'Peter': 'green', 'Paul': 'purple'})\n\nfig.add_vline(x=(1920 + 2000) / 2, line_dash=\"dash\", line_color=\"black\", annotation_text=\"Midpoint\")\n\nfig.show()"
  },
  {
    "objectID": "Projects/project1.html#questiontask-4",
    "href": "Projects/project1.html#questiontask-4",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nYou can see from the bar chart that the name Luke started to be more used after the first few movies of Star Wars came out. As time went on more and more people began to name their kids Luke and this can be largely due to the movies. When any of the Star Wars movies came out it would only take a year or two for the name to begin to spike in popularity.\n\n\nPlotting the name luke\nname = 'Luke'\n\nfiltered_data = df[df['name'] == name]\n\nfig = px.bar(filtered_data, x='year', y='Total',\n             title=f'Name Usage for {name} (1920-2000)',\n             labels={'year': 'Year', 'Total': 'Total Occurrences'})\n\nreference_years = [1977, 1980, 1983, 1999, 2002, 2005, 2015]\n\nfor year in reference_years:\n    fig.add_vline(x=year, line_dash=\"dash\", line_color=\"black\")\n\nfig.show()"
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "",
    "text": "So, I’ve been diving into this baseball data, and let me tell you, it’s pretty fascinating stuff! You wouldn’t believe the range in player salaries across teams it’s like peeking at their financial world. And then there are these player performance metrics, like batting average and wins, that give us a real insight into who’s making the biggest impact on the field.\n\n\nCode\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project3.html#elevator-pitch",
    "href": "Projects/project3.html#elevator-pitch",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "",
    "text": "So, I’ve been diving into this baseball data, and let me tell you, it’s pretty fascinating stuff! You wouldn’t believe the range in player salaries across teams it’s like peeking at their financial world. And then there are these player performance metrics, like batting average and wins, that give us a real insight into who’s making the biggest impact on the field.\n\n\nCode\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project3.html#questiontask-1",
    "href": "Projects/project3.html#questiontask-1",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe results of this query show that there haev not been many people from BYUI in the MLB. The ones that made it spent some time there and got paid quite a lot over their time. It also shows someone who was drafted but never got a salary.\n\n\nbaseball players who attended BYU-Idaho and their salary\nq = '''\nSELECT DISTINCT\n    a.playerID,\n    cp.schoolID,\n    s.salary,\n    s.yearID,\n    s.teamID\nFROM\n    Appearances a\nJOIN\n    CollegePlaying cp ON a.playerID = cp.playerID\nLEFT JOIN\n    Salaries s ON a.playerID = s.playerID\nJOIN\n    Schools sc ON cp.schoolID = sc.schoolID\nWHERE\n    sc.schoolID = 'idbyuid'\nORDER BY\n    s.salary DESC;\n\n'''\npd.read_sql_query(q,con)\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nsalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n4000000.0\n2014.0\nCHA\n\n\n1\nlindsma01\nidbyuid\n3600000.0\n2012.0\nBAL\n\n\n2\nlindsma01\nidbyuid\n2800000.0\n2011.0\nCOL\n\n\n3\nlindsma01\nidbyuid\n2300000.0\n2013.0\nCHA\n\n\n4\nlindsma01\nidbyuid\n1625000.0\n2010.0\nHOU\n\n\n5\nstephga01\nidbyuid\n1025000.0\n2001.0\nSLN\n\n\n6\nstephga01\nidbyuid\n900000.0\n2002.0\nSLN\n\n\n7\nstephga01\nidbyuid\n800000.0\n2003.0\nSLN\n\n\n8\nstephga01\nidbyuid\n550000.0\n2000.0\nSLN\n\n\n9\nlindsma01\nidbyuid\n410000.0\n2009.0\nFLO\n\n\n10\nlindsma01\nidbyuid\n395000.0\n2008.0\nFLO\n\n\n11\nlindsma01\nidbyuid\n380000.0\n2007.0\nFLO\n\n\n12\nstephga01\nidbyuid\n215000.0\n1999.0\nSLN\n\n\n13\nstephga01\nidbyuid\n185000.0\n1998.0\nPHI\n\n\n14\nstephga01\nidbyuid\n150000.0\n1997.0\nPHI\n\n\n15\ncatetr01\nidbyuid\nNaN\nNaN\nNone"
  },
  {
    "objectID": "Projects/project3.html#questiontask-2",
    "href": "Projects/project3.html#questiontask-2",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\nThe batting average is widely used in all leauges of baseball. It is easy to see that when players have only 1 at bat there is a good chance that the highest average is when they got on base on that one. The results are very similar when you bring it up to 10 at bats. The highest averages usually ahve the lowest at bats. WHen you look at the averages all time they are a lot lower at the top and show the better all time players.\n\n\nplayers with at least 1 at bat that year highest batting average to lowest\nq= '''\nSELECT\n    Batting.playerID,\n    Batting.yearID,\n    SUM(H) AS Hits,\n    SUM(AB) AS AtBats,\n    CAST(SUM(H) AS DECIMAL) / NULLIF(CAST(SUM(AB) AS DECIMAL), 0) AS BattingAverage\nFROM\n    Batting\nWHERE\n    AB &gt; 0\nGROUP BY\n    Batting.playerID,\n    Batting.yearID\nORDER BY\n    BattingAverage DESC,\n    playerID ASC\nLIMIT 5;\n'''\npd.read_sql_query(q,con)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nHits\nAtBats\nBattingAverage\n\n\n\n\n0\nabernte02\n1960\n1\n1\n1\n\n\n1\nabramge01\n1923\n1\n1\n1\n\n\n2\nacklefr01\n1964\n1\n1\n1\n\n\n3\nalanirj01\n2019\n1\n1\n1\n\n\n4\nalberan01\n2017\n1\n1\n1\n\n\n\n\n\n\n\n\n\nCode\n# at least 10 at bats that year\nq = '''\nSELECT\n    Batting.playerID,\n    Batting.yearID,\n    SUM(H) AS Hits,\n    SUM(AB) AS AtBats,\n    ROUND(SUM(H) * 1.0 / NULLIF(SUM(AB), 0), 3) AS BattingAverage\nFROM\n    Batting\nWHERE\n    AB &gt;= 10 \nGROUP BY\n    Batting.playerID,\n    Batting.yearID\nORDER BY\n    BattingAverage DESC,\n    Batting.playerID ASC\nLIMIT 5;\n'''\npd.read_sql_query(q,con)\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nHits\nAtBats\nBattingAverage\n\n\n\n\n0\nnymanny01\n1974\n9\n14\n0.643\n\n\n1\ncarsoma01\n2013\n7\n11\n0.636\n\n\n2\naltizda01\n1910\n6\n10\n0.600\n\n\n3\nsilvech01\n1948\n8\n14\n0.571\n\n\n4\npuccige01\n1930\n9\n16\n0.563\n\n\n\n\n\n\n\n\n\nCode\n# batting average for players over their entire careers\nq= '''\nSELECT \n    playerID,\n    SUM(H) AS TotalHits,\n    SUM(AB) AS TotalAtBats,\n    ROUND(SUM(H) * 1.0 / NULLIF(SUM(AB), 0), 3) AS CareerBattingAverage\nFROM \n    Batting\nGROUP BY \n    playerID\nHAVING \n    SUM(AB) &gt;= 100\nORDER BY \n    CareerBattingAverage DESC,\n    playerID ASC\nLIMIT 5;\n'''\npd.read_sql_query(q,con)\n\n\n\n\n\n\n\n\n\nplayerID\nTotalHits\nTotalAtBats\nCareerBattingAverage\n\n\n\n\n0\ncobbty01\n4189\n11436\n0.366\n\n\n1\nbarnero01\n860\n2391\n0.360\n\n\n2\nhornsro01\n2930\n8173\n0.358\n\n\n3\njacksjo01\n1772\n4981\n0.356\n\n\n4\nmeyerle01\n513\n1443\n0.356"
  },
  {
    "objectID": "Projects/project3.html#questiontask-3",
    "href": "Projects/project3.html#questiontask-3",
    "title": "Client Report - Project 3: Finding relationships in baseball.",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nThe two chosen baseball teams are the Seattle Mariners and the Las Angeles Angels (My favorite team and my best frineds favorite team). The data shows that over time the Mariners have been much more succesful in terms of total wins. This is also because the Mariners have more of a history than the Angels.\n\n\nCode\n# Angels Vs Mariners Total wins\nq = '''\nSELECT\n    teamID,\n    SUM(W) AS TotalWins\nFROM\n    Teams\nWHERE\n    teamID IN ('LAA', 'SEA')\nGROUP BY\n    teamID;\n\n'''\npd.read_sql_query(q,con)\n\n\n\n\n\n\n\n\n\nteamID\nTotalWins\n\n\n\n\n0\nLAA\n1605\n\n\n1\nSEA\n3219\n\n\n\n\n\n\n\n\n\nCode\ndf = pd.DataFrame({\n    'Team': ['Los Angeles Angels', 'Seattle Mariners'],\n    'Total Wins': [1605, 3219] \n})\n\nfig = px.bar(df, x='Team', y='Total Wins', text='Total Wins', title='Total Wins between Angels and Mariners')\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(yaxis=dict(title='Total Wins'))\nfig.show()"
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n\n                                                \nMy useless chart\n\n\n\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\nNot much of a table \n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0"
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n\n                                                \nMy useless chart\n\n\n\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\nNot much of a table \n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0"
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n\n                                                \nMy useless chart\n\n\n\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\nNot much of a table \n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0"
  },
  {
    "objectID": "Projects/W07 - U3 - Week B - Class Code Walkthrough 2.html",
    "href": "Projects/W07 - U3 - Week B - Class Code Walkthrough 2.html",
    "title": "W07.2 - U3 (Week B) - Class Code",
    "section": "",
    "text": "A further in-depth look at the functions you will be learning in this unit. Below is the code used in the video so you can follow along.\n\n# %%\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n\n# %%\n# careful to list your path to the file. Or save this code in the same folder as the sqlite file.\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\n# %%\n# allstar table\n\nq = '''\n    SELECT *\n    FROM AllstarFull\n    WHERE yearid &gt; 1999 \n        AND gp &lt;&gt; 1\n    LIMIT 5\n'''\n\ndf = pd.read_sql_query(q,con)\ndf\n\n\n\n\n\n\n\n\nID\nplayerID\nyearID\ngameNum\ngameID\nteamID\nteam_ID\nlgID\nGP\nstartingPos\n\n\n\n\n0\n3944\nfinlech01\n2000\n0\nNLS200007110\nCLE\n2334\nAL\n0\nNone\n\n\n1\n3953\nmartipe02\n2000\n0\nNLS200007110\nBOS\n2330\nAL\n0\nNone\n\n\n2\n3957\nramirma02\n2000\n0\nNLS200007110\nCLE\n2334\nAL\n0\nNone\n\n\n3\n3958\nripkeca01\n2000\n0\nNLS200007110\nBAL\n2329\nAL\n0\nNone\n\n\n4\n3960\nrodrial01\n2000\n0\nNLS200007110\nSEA\n2350\nAL\n0\nNone\n\n\n\n\n\n\n\n\n# %%\n# Can you use a groupby to get \n# the counts of players per year?\n\nq = '''\n    SELECT yearid, COUNT(*) as num_players\n    FROM AllstarFull\n    WHERE yearid &gt; 1999 \n        AND gp != 1\n    GROUP BY yearid\n    ORDER BY yearid\n    LIMIT 10\n'''\n\ndf = pd.read_sql_query(q,con)\ndf\n\n\n\n\n\n\n\n\nyearID\nnum_players\n\n\n\n\n0\n2000\n13\n\n\n1\n2001\n5\n\n\n2\n2002\n4\n\n\n3\n2003\n18\n\n\n4\n2004\n12\n\n\n5\n2005\n10\n\n\n6\n2006\n21\n\n\n7\n2007\n11\n\n\n8\n2008\n4\n\n\n9\n2009\n19\n\n\n\n\n\n\n\n\n# %%\n# join season game data and \n# calculate the total atbats and hits for each player by year.\nq = '''\nSELECT b.playerID\n,       SUM(b.ab)\n,       SUM(b.h)\nFROM AllstarFull as af\ninner join batting as b\nON b.playerid = af.playerid\nGROUP BY b.playerID\nHAVING SUM(b.ab)&lt;219469\nORDER BY SUM(b.ab) desc\nLIMIT 5\n'''\n\ndf = pd.read_sql_query(q,con)\ndf\n\n\n\n\n\n\n\n\nplayerID\nSUM(b.ab)\nSUM(b.h)\n\n\n\n\n0\nyastrca01\n215784\n61542\n\n\n1\nrobinbr01\n191772\n51264\n\n\n2\nkalinal01\n182088\n54126\n\n\n3\ncarewro01\n167670\n54954\n\n\n4\njeterde01\n156730\n48510\n\n\n\n\n\n\n\n\n# %%\n# Can you join the batting table and AllStar \n# information and keep only the at bats, \n# hits with the all star gp and gameid columns?\n\nq = '''\n    SELECT bp.playerid, \n        bp.yearid,\n        bp.ab, \n        bp.h, \n        asf.gp, \n        asf.gameid\n    FROM BattingPost as bp\n    LEFT JOIN AllstarFull as asf\n        ON  bp.playerid = asf.playerid AND\n            bp.yearid = asf.yearid\n    WHERE bp.yearid &gt; 1999\n        AND gp != 1\n        AND ab &gt; 0\n    LIMIT 15\n\n    '''\ndf = pd.read_sql_query(q,con)\ndf\n\n\n\n\n\n\n\n\nplayerID\nyearID\nAB\nH\nGP\ngameID\n\n\n\n\n0\nrodrial01\n2000\n22\n9\n0\nNLS200007110\n\n\n1\nrodrial01\n2000\n13\n4\n0\nNLS200007110\n\n\n2\nmcgwima01\n2000\n2\n0\n0\nNLS200007110\n\n\n3\npiazzmi01\n2000\n17\n7\n0\nNLS200007110\n\n\n4\nmaddugr01\n2000\n1\n0\n0\nNLS200007110\n\n\n5\nmcgwima01\n2000\n2\n1\n0\nNLS200007110\n\n\n6\nbondsba01\n2000\n17\n3\n0\nNLS200007110\n\n\n7\npiazzmi01\n2000\n14\n3\n0\nNLS200007110\n\n\n8\npiazzmi01\n2000\n22\n6\n0\nNLS200007110\n\n\n9\nschilcu01\n2001\n4\n1\n0\nALS200107100\n\n\n10\nschilcu01\n2001\n5\n0\n0\nALS200107100\n\n\n11\nschilcu01\n2001\n6\n0\n0\nALS200107100\n\n\n12\nmorrima01\n2002\n4\n0\n0\nNLS200207090\n\n\n13\nglavito02\n2002\n2\n1\n0\nNLS200207090\n\n\n14\njohnsra05\n2002\n2\n0\n0\nNLS200207090\n\n\n\n\n\n\n\n\n# %%\n# Let's build the final table\n\n# Which year had the most players players selected as All Stars \n# but didn't play in the All Star game after 1999?\n\n# provide a summary of how many games, hits, and at bats \n# occured by those players had in that years post season.\n\nq = '''\n    SELECT bp.yearid, \n        sum(ab) as ab, \n        sum(h) as h,\n        sum(g) as games, \n        count(ab) as num_players, \n        asf.gp, \n        asf.gameid\n    FROM BattingPost as bp\n    JOIN AllstarFull as asf\n        ON  bp.playerid = asf.playerid AND\n            bp.yearid = asf.yearid\n    WHERE bp.yearid &gt; 1999\n        AND asf.gp != 1\n        AND bp.ab &gt; 0\n    GROUP BY bp.yearid\n    ORDER BY bp.yearid\n    LIMIT 15\n    '''\ndf = pd.read_sql_query(q,con)\ndf\n\n\n\n\n\n\n\n\nyearID\nab\nh\ngames\nnum_players\nGP\ngameID\n\n\n\n\n0\n2000\n110\n33\n35\n9\n0\nNLS200007110\n\n\n1\n2001\n15\n1\n6\n3\n0\nALS200107100\n\n\n2\n2002\n12\n2\n6\n4\n0\nNLS200207090\n\n\n3\n2003\n107\n31\n33\n8\n0\nALS200307150\n\n\n4\n2006\n136\n30\n40\n12\n0\nNLS200607110\n\n\n5\n2007\n4\n1\n2\n2\n0\nNLS200707100\n\n\n6\n2008\n57\n9\n14\n3\n0\nALS200807150\n\n\n7\n2009\n46\n11\n15\n4\n0\nNLS200907140\n\n\n8\n2010\n111\n21\n34\n10\n0\nALS201007130\n\n\n9\n2011\n99\n19\n26\n6\n0\nNLS201107120\n\n\n10\n2012\n70\n20\n20\n5\n0\nALS201207100\n\n\n11\n2013\n57\n8\n19\n8\n0\nNLS201307160\n\n\n12\n2014\n145\n31\n45\n17\n0\nALS201407150\n\n\n13\n2015\n113\n27\n32\n7\n0\nNLS201507140\n\n\n14\n2016\n95\n21\n29\n11\n0\nALS201607120\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Caleb Tubbs’s CV",
    "section": "",
    "text": "Brigham Young University - Idaho Student\n\ncalebtubbs17@gmail.com\n\n\n\nAttending Brigham Young University - Idaho Computer Science Major Data Science Minor\n\n\nProgramming in Python, Mysql, Javascript, R, Html, Css\n\n\n\nSoftware engineering Computer Science\n\n\n\n\n2022-Current Brigham Young University-Idaho, Rexburg.\n\n\n\nEagle Scout\n\n\n\n2020 Helena Motors, Helena, MT\n\nPorter\nOrganized where cars would be placed in the parking lot and helped with cleaning and other responsibilities dealing with the customers.\n\n2021-2022 Progrexion, Rexburg, Id\n\nSales agent\nTook phone calls from potential clients and turned the calls into sales.\n\n2023 Cedars Lodge, Ketchikan, AK\n\nFish Cutter\nLong hours of hard labor cutting fish for up to 14 hours a day. It made my work ethic and drive become much stronger"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Caleb Tubbs’s CV",
    "section": "",
    "text": "Attending Brigham Young University - Idaho Computer Science Major Data Science Minor\n\n\nProgramming in Python, Mysql, Javascript, R, Html, Css\n\n\n\nSoftware engineering Computer Science"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Caleb Tubbs’s CV",
    "section": "",
    "text": "2022-Current Brigham Young University-Idaho, Rexburg."
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Caleb Tubbs’s CV",
    "section": "",
    "text": "Eagle Scout"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Caleb Tubbs’s CV",
    "section": "",
    "text": "2020 Helena Motors, Helena, MT\n\nPorter\nOrganized where cars would be placed in the parking lot and helped with cleaning and other responsibilities dealing with the customers.\n\n2021-2022 Progrexion, Rexburg, Id\n\nSales agent\nTook phone calls from potential clients and turned the calls into sales.\n\n2023 Cedars Lodge, Ketchikan, AK\n\nFish Cutter\nLong hours of hard labor cutting fish for up to 14 hours a day. It made my work ethic and drive become much stronger"
  }
]